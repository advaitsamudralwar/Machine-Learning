{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbkeDZsJzNHp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import cvxpy as cp\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBdVx02I1HBk",
        "outputId": "929b24f9-cf9e-49eb-a574-048b2735c273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "Epoch:  1\n",
            "Epoch:  2\n",
            "Epoch:  3\n",
            "Epoch:  4\n",
            "Epoch:  5\n",
            "Epoch:  6\n",
            "Epoch:  7\n",
            "Epoch:  8\n",
            "Epoch:  9\n",
            "Epoch:  10\n",
            "Epoch:  11\n",
            "Epoch:  12\n",
            "Epoch:  13\n",
            "Epoch:  14\n",
            "Epoch:  15\n",
            "Epoch:  16\n",
            "Epoch:  17\n",
            "Epoch:  18\n",
            "Epoch:  19\n",
            "Epoch:  20\n",
            "Epoch:  21\n",
            "Epoch:  22\n",
            "Epoch:  23\n",
            "Epoch:  24\n",
            "Epoch:  25\n",
            "Epoch:  26\n",
            "Epoch:  27\n",
            "Epoch:  28\n",
            "Epoch:  29\n",
            "Epoch:  30\n",
            "Epoch:  31\n",
            "Epoch:  32\n",
            "Epoch:  33\n",
            "Epoch:  34\n",
            "Epoch:  35\n",
            "Epoch:  36\n",
            "Epoch:  37\n",
            "Epoch:  38\n",
            "Epoch:  39\n",
            "Epoch:  40\n",
            "Epoch:  41\n",
            "Epoch:  42\n",
            "Epoch:  43\n",
            "Epoch:  44\n",
            "Epoch:  45\n",
            "Epoch:  46\n",
            "Epoch:  47\n",
            "Epoch:  48\n",
            "Epoch:  49\n",
            "Epoch:  50\n",
            "Epoch:  51\n",
            "Epoch:  52\n",
            "Epoch:  53\n",
            "Epoch:  54\n",
            "Epoch:  55\n",
            "Epoch:  56\n",
            "Epoch:  57\n",
            "Epoch:  58\n",
            "Epoch:  59\n",
            "Epoch:  60\n",
            "Epoch:  61\n",
            "Epoch:  62\n",
            "Epoch:  63\n",
            "Epoch:  64\n",
            "Epoch:  65\n",
            "Epoch:  66\n",
            "Epoch:  67\n",
            "Epoch:  68\n",
            "Epoch:  69\n",
            "Epoch:  70\n",
            "Epoch:  71\n",
            "Epoch:  72\n",
            "Epoch:  73\n",
            "Epoch:  74\n",
            "Epoch:  75\n",
            "Epoch:  76\n",
            "Epoch:  77\n",
            "Epoch:  78\n",
            "Epoch:  79\n",
            "Epoch:  80\n",
            "Epoch:  81\n",
            "Epoch:  82\n",
            "Epoch:  83\n",
            "Epoch:  84\n",
            "Epoch:  85\n",
            "Epoch:  86\n",
            "Epoch:  87\n",
            "Epoch:  88\n",
            "Epoch:  89\n",
            "Epoch:  90\n",
            "Epoch:  91\n",
            "Epoch:  92\n",
            "Epoch:  93\n",
            "Epoch:  94\n",
            "Epoch:  95\n",
            "Epoch:  96\n",
            "Epoch:  97\n",
            "Epoch:  98\n",
            "Epoch:  99\n",
            "Epoch:  100\n",
            "Epoch:  101\n",
            "Epoch:  102\n",
            "Epoch:  103\n",
            "Epoch:  104\n",
            "Epoch:  105\n",
            "Epoch:  106\n",
            "Epoch:  107\n",
            "Epoch:  108\n",
            "Epoch:  109\n",
            "Epoch:  110\n",
            "Epoch:  111\n",
            "Epoch:  112\n",
            "Epoch:  113\n",
            "Epoch:  114\n",
            "Epoch:  115\n",
            "Epoch:  116\n",
            "Epoch:  117\n",
            "Epoch:  118\n",
            "Epoch:  119\n",
            "Epoch:  120\n",
            "Epoch:  121\n",
            "Epoch:  122\n",
            "Epoch:  123\n",
            "Epoch:  124\n",
            "Epoch:  125\n",
            "Epoch:  126\n",
            "Epoch:  127\n",
            "Epoch:  128\n",
            "Epoch:  129\n",
            "Epoch:  130\n",
            "Epoch:  131\n",
            "Epoch:  132\n",
            "Epoch:  133\n",
            "Epoch:  134\n",
            "Epoch:  135\n",
            "Epoch:  136\n",
            "Epoch:  137\n",
            "Epoch:  138\n",
            "Epoch:  139\n",
            "Epoch:  140\n",
            "Epoch:  141\n",
            "Epoch:  142\n",
            "Epoch:  143\n",
            "Epoch:  144\n",
            "Epoch:  145\n",
            "Epoch:  146\n",
            "Epoch:  147\n",
            "Epoch:  148\n",
            "Epoch:  149\n",
            "Epoch:  150\n",
            "Epoch:  151\n",
            "Epoch:  152\n",
            "Epoch:  153\n",
            "Epoch:  154\n",
            "Epoch:  155\n",
            "Epoch:  156\n",
            "Epoch:  157\n",
            "Epoch:  158\n",
            "Epoch:  159\n",
            "Epoch:  160\n",
            "Epoch:  161\n",
            "Epoch:  162\n",
            "Epoch:  163\n",
            "Epoch:  164\n",
            "Epoch:  165\n",
            "Epoch:  166\n",
            "Epoch:  167\n",
            "Epoch:  168\n",
            "Epoch:  169\n",
            "Epoch:  170\n",
            "Epoch:  171\n",
            "Epoch:  172\n",
            "Epoch:  173\n",
            "Epoch:  174\n",
            "Epoch:  175\n",
            "Epoch:  176\n",
            "Epoch:  177\n",
            "Epoch:  178\n",
            "Epoch:  179\n",
            "Epoch:  180\n",
            "Epoch:  181\n",
            "Epoch:  182\n",
            "Epoch:  183\n",
            "Epoch:  184\n",
            "Epoch:  185\n",
            "Epoch:  186\n",
            "Epoch:  187\n",
            "Epoch:  188\n",
            "Epoch:  189\n",
            "Epoch:  190\n",
            "Epoch:  191\n",
            "Epoch:  192\n",
            "Epoch:  193\n",
            "Epoch:  194\n",
            "Epoch:  195\n",
            "Epoch:  196\n",
            "Epoch:  197\n",
            "Epoch:  198\n",
            "Epoch:  199\n",
            "Epoch:  200\n",
            "Epoch:  201\n",
            "Epoch:  202\n",
            "Epoch:  203\n",
            "Epoch:  204\n",
            "Epoch:  205\n",
            "Epoch:  206\n",
            "Epoch:  207\n",
            "Epoch:  208\n",
            "Epoch:  209\n",
            "Epoch:  210\n",
            "Epoch:  211\n",
            "Epoch:  212\n",
            "Epoch:  213\n",
            "Epoch:  214\n",
            "Epoch:  215\n",
            "Epoch:  216\n",
            "Epoch:  217\n",
            "Epoch:  218\n",
            "Epoch:  219\n",
            "Epoch:  220\n",
            "Epoch:  221\n",
            "Epoch:  222\n",
            "Epoch:  223\n",
            "Epoch:  224\n",
            "Epoch:  225\n",
            "Epoch:  226\n",
            "Epoch:  227\n",
            "Epoch:  228\n",
            "Epoch:  229\n",
            "Epoch:  230\n",
            "Epoch:  231\n",
            "Epoch:  232\n",
            "Epoch:  233\n",
            "Epoch:  234\n",
            "Epoch:  235\n",
            "Epoch:  236\n",
            "Epoch:  237\n",
            "Epoch:  238\n",
            "Epoch:  239\n",
            "Epoch:  240\n",
            "Epoch:  241\n",
            "Epoch:  242\n",
            "Epoch:  243\n",
            "Epoch:  244\n",
            "Epoch:  245\n",
            "Epoch:  246\n",
            "Epoch:  247\n",
            "Epoch:  248\n",
            "Epoch:  249\n",
            "Epoch:  250\n",
            "Epoch:  251\n",
            "Epoch:  252\n",
            "Epoch:  253\n",
            "Epoch:  254\n",
            "Epoch:  255\n",
            "Epoch:  256\n",
            "Epoch:  257\n",
            "Epoch:  258\n",
            "Epoch:  259\n",
            "Epoch:  260\n",
            "Epoch:  261\n",
            "Epoch:  262\n",
            "Epoch:  263\n",
            "Epoch:  264\n",
            "Epoch:  265\n",
            "Epoch:  266\n",
            "Epoch:  267\n",
            "Epoch:  268\n",
            "Epoch:  269\n",
            "Epoch:  270\n",
            "Epoch:  271\n",
            "Epoch:  272\n",
            "Epoch:  273\n",
            "Epoch:  274\n",
            "Epoch:  275\n",
            " Gradient is below threshold value,breaking 0.009984696847582698\n",
            "Epoch:  0\n",
            "Epoch:  1\n",
            "Epoch:  2\n",
            "Epoch:  3\n",
            "Epoch:  4\n",
            "Epoch:  5\n",
            "Epoch:  6\n",
            "Epoch:  7\n",
            "Epoch:  8\n",
            "Epoch:  9\n",
            "Epoch:  10\n",
            "Epoch:  11\n",
            "Epoch:  12\n",
            "Epoch:  13\n",
            "Epoch:  14\n",
            "Epoch:  15\n",
            "Epoch:  16\n",
            "Epoch:  17\n",
            "Epoch:  18\n",
            "Epoch:  19\n",
            "Epoch:  20\n",
            "Epoch:  21\n",
            "Epoch:  22\n",
            "Epoch:  23\n",
            "Epoch:  24\n",
            "Epoch:  25\n",
            "Epoch:  26\n",
            "Epoch:  27\n",
            "Epoch:  28\n",
            "Epoch:  29\n",
            "Epoch:  30\n",
            "Epoch:  31\n",
            "Epoch:  32\n",
            "Epoch:  33\n",
            "Epoch:  34\n",
            "Epoch:  35\n",
            "Epoch:  36\n",
            "Epoch:  37\n",
            "Epoch:  38\n",
            "Epoch:  39\n",
            "Epoch:  40\n",
            "Epoch:  41\n",
            "Epoch:  42\n",
            "Epoch:  43\n",
            "Epoch:  44\n",
            "Epoch:  45\n",
            "Epoch:  46\n",
            "Epoch:  47\n",
            "Epoch:  48\n",
            "Epoch:  49\n",
            "Epoch:  50\n",
            "Epoch:  51\n",
            "Epoch:  52\n",
            "Epoch:  53\n",
            "Epoch:  54\n",
            "Epoch:  55\n",
            "Epoch:  56\n",
            "Epoch:  57\n",
            "Epoch:  58\n",
            "Epoch:  59\n",
            "Epoch:  60\n",
            "Epoch:  61\n",
            "Epoch:  62\n",
            "Epoch:  63\n",
            "Epoch:  64\n",
            "Epoch:  65\n",
            "Epoch:  66\n",
            "Epoch:  67\n",
            "Epoch:  68\n",
            "Epoch:  69\n",
            "Epoch:  70\n",
            "Epoch:  71\n",
            "Epoch:  72\n",
            "Epoch:  73\n",
            "Epoch:  74\n",
            "Epoch:  75\n",
            "Epoch:  76\n",
            "Epoch:  77\n",
            "Epoch:  78\n",
            "Epoch:  79\n",
            "Epoch:  80\n",
            "Epoch:  81\n",
            "Epoch:  82\n",
            "Epoch:  83\n",
            "Epoch:  84\n",
            "Epoch:  85\n",
            "Epoch:  86\n",
            "Epoch:  87\n",
            "Epoch:  88\n",
            "Epoch:  89\n",
            "Epoch:  90\n",
            "Epoch:  91\n",
            "Epoch:  92\n",
            "Epoch:  93\n",
            "Epoch:  94\n",
            "Epoch:  95\n",
            "Epoch:  96\n",
            "Epoch:  97\n",
            "Epoch:  98\n",
            "Epoch:  99\n",
            "Epoch:  100\n",
            "Epoch:  101\n",
            "Epoch:  102\n",
            "Epoch:  103\n",
            "Epoch:  104\n",
            "Epoch:  105\n",
            "Epoch:  106\n",
            "Epoch:  107\n",
            "Epoch:  108\n",
            "Epoch:  109\n",
            "Epoch:  110\n",
            "Epoch:  111\n",
            "Epoch:  112\n",
            "Epoch:  113\n",
            "Epoch:  114\n",
            "Epoch:  115\n",
            "Epoch:  116\n",
            "Epoch:  117\n",
            "Epoch:  118\n",
            "Epoch:  119\n",
            "Epoch:  120\n",
            "Epoch:  121\n",
            "Epoch:  122\n",
            "Epoch:  123\n",
            "Epoch:  124\n",
            "Epoch:  125\n",
            "Epoch:  126\n",
            "Epoch:  127\n",
            "Epoch:  128\n",
            "Epoch:  129\n",
            "Epoch:  130\n",
            "Epoch:  131\n",
            "Epoch:  132\n",
            "Epoch:  133\n",
            "Epoch:  134\n",
            "Epoch:  135\n",
            "Epoch:  136\n",
            "Epoch:  137\n",
            "Epoch:  138\n",
            "Epoch:  139\n",
            "Epoch:  140\n",
            "Epoch:  141\n",
            "Epoch:  142\n",
            "Epoch:  143\n",
            "Epoch:  144\n",
            "Epoch:  145\n",
            "Epoch:  146\n",
            "Epoch:  147\n",
            "Epoch:  148\n",
            "Epoch:  149\n",
            "Epoch:  150\n",
            "Epoch:  151\n",
            "Epoch:  152\n",
            "Epoch:  153\n",
            "Epoch:  154\n",
            "Epoch:  155\n",
            "Epoch:  156\n",
            "Epoch:  157\n",
            "Epoch:  158\n",
            "Epoch:  159\n",
            "Epoch:  160\n",
            "Epoch:  161\n",
            "Epoch:  162\n",
            "Epoch:  163\n",
            "Epoch:  164\n",
            "Epoch:  165\n",
            "Epoch:  166\n",
            "Epoch:  167\n",
            "Epoch:  168\n",
            "Epoch:  169\n",
            "Epoch:  170\n",
            "Epoch:  171\n",
            "Epoch:  172\n",
            "Epoch:  173\n",
            "Epoch:  174\n",
            "Epoch:  175\n",
            "Epoch:  176\n",
            "Epoch:  177\n",
            "Epoch:  178\n",
            "Epoch:  179\n",
            "Epoch:  180\n",
            "Epoch:  181\n",
            "Epoch:  182\n",
            "Epoch:  183\n",
            "Epoch:  184\n",
            "Epoch:  185\n",
            "Epoch:  186\n",
            "Epoch:  187\n",
            "Epoch:  188\n",
            "Epoch:  189\n",
            "Epoch:  190\n",
            "Epoch:  191\n",
            "Epoch:  192\n",
            "Epoch:  193\n",
            "Epoch:  194\n",
            "Epoch:  195\n",
            "Epoch:  196\n",
            "Epoch:  197\n",
            "Epoch:  198\n",
            "Epoch:  199\n",
            "Epoch:  200\n",
            "Epoch:  201\n",
            "Epoch:  202\n",
            "Epoch:  203\n",
            "Epoch:  204\n",
            "Epoch:  205\n",
            "Epoch:  206\n",
            "Epoch:  207\n",
            "Epoch:  208\n",
            "Epoch:  209\n",
            "Epoch:  210\n",
            "Epoch:  211\n",
            "Epoch:  212\n",
            "Epoch:  213\n",
            "Epoch:  214\n",
            "Epoch:  215\n",
            "Epoch:  216\n",
            "Epoch:  217\n",
            "Epoch:  218\n",
            "Epoch:  219\n",
            "Epoch:  220\n",
            "Epoch:  221\n",
            "Epoch:  222\n",
            "Epoch:  223\n",
            "Epoch:  224\n",
            "Epoch:  225\n",
            "Epoch:  226\n",
            "Epoch:  227\n",
            "Epoch:  228\n",
            "Epoch:  229\n",
            "Epoch:  230\n",
            "Epoch:  231\n",
            "Epoch:  232\n",
            "Epoch:  233\n",
            "Epoch:  234\n",
            "Epoch:  235\n",
            "Epoch:  236\n",
            "Epoch:  237\n",
            "Epoch:  238\n",
            "Epoch:  239\n",
            "Epoch:  240\n",
            "Epoch:  241\n",
            "Epoch:  242\n",
            "Epoch:  243\n",
            "Epoch:  244\n",
            "Epoch:  245\n",
            "Epoch:  246\n",
            "Epoch:  247\n",
            "Epoch:  248\n",
            "Epoch:  249\n",
            "Epoch:  250\n",
            "Epoch:  251\n",
            "Epoch:  252\n",
            "Epoch:  253\n",
            "Epoch:  254\n",
            "Epoch:  255\n",
            "Epoch:  256\n",
            "Epoch:  257\n",
            "Epoch:  258\n",
            "Epoch:  259\n",
            "Epoch:  260\n",
            "Epoch:  261\n",
            "Epoch:  262\n",
            "Epoch:  263\n",
            "Epoch:  264\n",
            "Epoch:  265\n",
            "Epoch:  266\n",
            "Epoch:  267\n",
            "Epoch:  268\n",
            "Epoch:  269\n",
            "Epoch:  270\n",
            "Epoch:  271\n",
            "Epoch:  272\n",
            "Epoch:  273\n",
            "Epoch:  274\n",
            "Epoch:  275\n",
            "Epoch:  276\n",
            "Epoch:  277\n",
            "Epoch:  278\n",
            "Epoch:  279\n",
            "Epoch:  280\n",
            "Epoch:  281\n",
            "Epoch:  282\n",
            "Epoch:  283\n",
            "Epoch:  284\n",
            "Epoch:  285\n",
            "Epoch:  286\n",
            "Epoch:  287\n",
            "Epoch:  288\n",
            "Epoch:  289\n",
            "Epoch:  290\n",
            "Epoch:  291\n",
            "Epoch:  292\n",
            "Epoch:  293\n",
            "Epoch:  294\n",
            "Epoch:  295\n",
            "Epoch:  296\n",
            "Epoch:  297\n",
            "Epoch:  298\n",
            "Epoch:  299\n",
            "Epoch:  300\n",
            "Epoch:  301\n",
            "Epoch:  302\n",
            "Epoch:  303\n",
            "Epoch:  304\n",
            "Epoch:  305\n",
            "Epoch:  306\n",
            "Epoch:  307\n",
            "Epoch:  308\n",
            "Epoch:  309\n",
            "Epoch:  310\n",
            "Epoch:  311\n",
            "Epoch:  312\n",
            "Epoch:  313\n",
            "Epoch:  314\n",
            "Epoch:  315\n",
            "Epoch:  316\n",
            "Epoch:  317\n",
            "Epoch:  318\n",
            "Epoch:  319\n",
            "Epoch:  320\n",
            "Epoch:  321\n",
            "Epoch:  322\n",
            "Epoch:  323\n",
            "Epoch:  324\n",
            "Epoch:  325\n",
            "Epoch:  326\n",
            "Epoch:  327\n",
            "Epoch:  328\n",
            "Epoch:  329\n",
            "Epoch:  330\n",
            "Epoch:  331\n",
            "Epoch:  332\n",
            "Epoch:  333\n",
            "Epoch:  334\n",
            "Epoch:  335\n",
            "Epoch:  336\n",
            "Epoch:  337\n",
            "Epoch:  338\n",
            "Epoch:  339\n",
            "Epoch:  340\n",
            "Epoch:  341\n",
            "Epoch:  342\n",
            "Epoch:  343\n",
            "Epoch:  344\n",
            "Epoch:  345\n",
            "Epoch:  346\n",
            "Epoch:  347\n",
            "Epoch:  348\n",
            "Epoch:  349\n",
            "Epoch:  350\n",
            "Epoch:  351\n",
            "Epoch:  352\n",
            "Epoch:  353\n",
            "Epoch:  354\n",
            "Epoch:  355\n",
            "Epoch:  356\n",
            "Epoch:  357\n",
            "Epoch:  358\n",
            "Epoch:  359\n",
            "Epoch:  360\n",
            "Epoch:  361\n",
            "Epoch:  362\n",
            "Epoch:  363\n",
            "Epoch:  364\n",
            "Epoch:  365\n",
            "Epoch:  366\n",
            "Epoch:  367\n",
            "Epoch:  368\n",
            "Epoch:  369\n",
            "Epoch:  370\n",
            "Epoch:  371\n",
            "Epoch:  372\n",
            "Epoch:  373\n",
            "Epoch:  374\n",
            "Epoch:  375\n",
            "Epoch:  376\n",
            "Epoch:  377\n",
            "Epoch:  378\n",
            "Epoch:  379\n",
            "Epoch:  380\n",
            "Epoch:  381\n",
            "Epoch:  382\n",
            "Epoch:  383\n",
            "Epoch:  384\n",
            "Epoch:  385\n",
            "Epoch:  386\n",
            "Epoch:  387\n",
            "Epoch:  388\n",
            "Epoch:  389\n",
            "Epoch:  390\n",
            "Epoch:  391\n",
            "Epoch:  392\n",
            "Epoch:  393\n",
            "Epoch:  394\n",
            "Epoch:  395\n",
            "Epoch:  396\n",
            "Epoch:  397\n",
            "Epoch:  398\n",
            "Epoch:  399\n",
            "Epoch:  400\n",
            "Epoch:  401\n",
            "Epoch:  402\n",
            "Epoch:  403\n",
            "Epoch:  404\n",
            "Epoch:  405\n",
            "Epoch:  406\n",
            "Epoch:  407\n",
            "Epoch:  408\n",
            "Epoch:  409\n",
            "Epoch:  410\n",
            "Epoch:  411\n",
            "Epoch:  412\n",
            "Epoch:  413\n",
            "Epoch:  414\n",
            "Epoch:  415\n",
            "Epoch:  416\n",
            "Epoch:  417\n",
            "Epoch:  418\n",
            "Epoch:  419\n",
            "Epoch:  420\n",
            "Epoch:  421\n",
            "Epoch:  422\n",
            "Epoch:  423\n",
            "Epoch:  424\n",
            "Epoch:  425\n",
            "Epoch:  426\n",
            "Epoch:  427\n",
            "Epoch:  428\n",
            "Epoch:  429\n",
            "Epoch:  430\n",
            "Epoch:  431\n",
            "Epoch:  432\n",
            "Epoch:  433\n",
            "Epoch:  434\n",
            "Epoch:  435\n",
            "Epoch:  436\n",
            "Epoch:  437\n",
            "Epoch:  438\n",
            "Epoch:  439\n",
            "Epoch:  440\n",
            "Epoch:  441\n",
            "Epoch:  442\n",
            "Epoch:  443\n",
            "Epoch:  444\n",
            "Epoch:  445\n",
            "Epoch:  446\n",
            "Epoch:  447\n",
            "Epoch:  448\n",
            "Epoch:  449\n",
            "Epoch:  450\n",
            "Epoch:  451\n",
            "Epoch:  452\n",
            "Epoch:  453\n",
            "Epoch:  454\n",
            "Epoch:  455\n",
            "Epoch:  456\n",
            "Epoch:  457\n",
            "Epoch:  458\n",
            "Epoch:  459\n",
            "Epoch:  460\n",
            "Epoch:  461\n",
            "Epoch:  462\n",
            "Epoch:  463\n",
            "Epoch:  464\n",
            "Epoch:  465\n",
            "Epoch:  466\n",
            "Epoch:  467\n",
            "Epoch:  468\n",
            "Epoch:  469\n",
            "Epoch:  470\n",
            "Epoch:  471\n",
            "Epoch:  472\n",
            "Epoch:  473\n",
            "Epoch:  474\n",
            "Epoch:  475\n",
            "Epoch:  476\n",
            "Epoch:  477\n",
            "Epoch:  478\n",
            "Epoch:  479\n",
            "Epoch:  480\n",
            "Epoch:  481\n",
            "Epoch:  482\n",
            "Epoch:  483\n",
            "Epoch:  484\n",
            "Epoch:  485\n",
            "Epoch:  486\n",
            "Epoch:  487\n",
            "Epoch:  488\n",
            "Epoch:  489\n",
            "Epoch:  490\n",
            "Epoch:  491\n",
            "Epoch:  492\n",
            "Epoch:  493\n",
            "Epoch:  494\n",
            "Epoch:  495\n",
            "Epoch:  496\n",
            "Epoch:  497\n",
            "Epoch:  498\n",
            "Epoch:  499\n",
            "Epoch:  500\n",
            "Epoch:  501\n",
            "Epoch:  502\n",
            "Epoch:  503\n",
            "Epoch:  504\n",
            "Epoch:  505\n",
            "Epoch:  506\n",
            "Epoch:  507\n",
            "Epoch:  508\n",
            "Epoch:  509\n",
            "Epoch:  510\n",
            "Epoch:  511\n",
            "Epoch:  512\n",
            "Epoch:  513\n",
            "Epoch:  514\n",
            "Epoch:  515\n",
            "Epoch:  516\n",
            "Epoch:  517\n",
            "Epoch:  518\n",
            "Epoch:  519\n",
            "Epoch:  520\n",
            "Epoch:  521\n",
            "Epoch:  522\n",
            "Epoch:  523\n",
            "Epoch:  524\n",
            "Epoch:  525\n",
            "Epoch:  526\n",
            "Epoch:  527\n",
            "Epoch:  528\n",
            "Epoch:  529\n",
            "Epoch:  530\n",
            "Epoch:  531\n",
            "Epoch:  532\n",
            "Epoch:  533\n",
            "Epoch:  534\n",
            "Epoch:  535\n",
            "Epoch:  536\n",
            "Epoch:  537\n",
            "Epoch:  538\n",
            "Epoch:  539\n",
            "Epoch:  540\n",
            "Epoch:  541\n",
            "Epoch:  542\n",
            "Epoch:  543\n",
            "Epoch:  544\n",
            "Epoch:  545\n",
            "Epoch:  546\n",
            "Epoch:  547\n",
            "Epoch:  548\n",
            "Epoch:  549\n",
            "Epoch:  550\n",
            "Epoch:  551\n",
            "Epoch:  552\n",
            "Epoch:  553\n",
            "Epoch:  554\n",
            "Epoch:  555\n",
            "Epoch:  556\n",
            "Epoch:  557\n",
            "Epoch:  558\n",
            "Epoch:  559\n",
            "Epoch:  560\n",
            "Epoch:  561\n",
            "Epoch:  562\n",
            "Epoch:  563\n",
            "Epoch:  564\n",
            "Epoch:  565\n",
            "Epoch:  566\n",
            "Epoch:  567\n",
            "Epoch:  568\n",
            "Epoch:  569\n",
            "Epoch:  570\n",
            "Epoch:  571\n",
            "Epoch:  572\n",
            "Epoch:  573\n",
            "Epoch:  574\n",
            "Epoch:  575\n",
            "Epoch:  576\n",
            "Epoch:  577\n",
            "Epoch:  578\n",
            "Epoch:  579\n",
            "Epoch:  580\n",
            "Epoch:  581\n",
            "Epoch:  582\n",
            "Epoch:  583\n",
            "Epoch:  584\n",
            "Epoch:  585\n",
            "Epoch:  586\n",
            "Epoch:  587\n",
            "Epoch:  588\n",
            "Epoch:  589\n",
            "Epoch:  590\n",
            "Epoch:  591\n",
            "Epoch:  592\n",
            "Epoch:  593\n",
            "Epoch:  594\n",
            "Epoch:  595\n",
            "Epoch:  596\n",
            "Epoch:  597\n",
            "Epoch:  598\n",
            " Gradient is below threshold value,breaking 0.0099919641324066\n",
            "Training loss:  0.0\n",
            "Testing loss:  0.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Task 1.1) Batch Gradient Descent\n",
        "#load data and preprocess data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "#choose pictures of number 2 and number 6\n",
        "(train_images, train_labels), (test_images,test_labels) =mnist.load_data()#include all numbers from 0 to 9 index_train=p.where((train labels ==2) | (train labels ==6))#index of numbers 3 and 5 in training data index_test=p.where((test_labels ==2) | (test_labels ==6))#index of numbers 3 and 5 in test data train images 26=train images [index train]\n",
        "index_train=np.where((train_labels == 2) | (train_labels == 6))#index of numbers 3 and 5 in training data\n",
        "index_test=np.where((test_labels ==2) | (test_labels ==6))#index of numbers 3 and 5 in test data\n",
        "train_images_26=train_images[index_train]\n",
        "train_images_26=train_images_26.reshape ((len (train_images_26), train_images_26[1].size)) #label of number 2: -1; label of number 6: +1 train_labels_26=train_labels[index_train].astype('int')\n",
        "#label of number 2: -1; label of number 6: +1 \n",
        "train_labels_26=train_labels[index_train].astype('int')\n",
        "test_images_26=test_images[index_test]\n",
        "test_images_26=test_images_26.reshape((len(test_images_26), train_images_26[1].size)) \n",
        "test_labels_26=test_labels[index_test].astype('int')\n",
        "#change labels from '2' and '6' to '-1' and '+1' \n",
        "train_labels_26[np.where(train_labels_26==2)]= 0\n",
        "train_labels_26[np.where(train_labels_26==6)]=1\n",
        "test_labels_26[np.where(test_labels_26==2)]= 0\n",
        "test_labels_26[np.where(test_labels_26==6)]=1\n",
        "#Normalize \n",
        "train_images_26_w_dummy=np.insert(train_images_26,784,1,axis=1)/255\n",
        "test_images_26_w_dummy=np.insert(test_images_26,784,1,axis=1)/255\n",
        "#choose a subset of the enntire training dataset\n",
        "train_images_26_w_dummy=train_images_26_w_dummy[range(1000)]\n",
        "train_labels_26_w_dummy=train_labels_26[range(1000)]\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.metrics.pairwise import pairwise_kernels\n",
        "K_train=rbf_kernel(X1,gamma=10)\n",
        "K_train_test=rbf_kernel(X1,X1_test,gamma=10)\n",
        "\n",
        "rows = train_images_26_w_dummy.shape[0]\n",
        "colms = train_images_26_w_dummy.shape[1]\n",
        "\n",
        "alpha = 0.4\n",
        "bgd_conv = 0.01\n",
        "epoch = 0\n",
        "theta = np.zeros(colms)\n",
        "batch = 50\n",
        "\n",
        "#-------------BGD on train data\n",
        "while(True):\n",
        "  #slice training data in batches of 50\n",
        "  for i in range(0, rows, batch):\n",
        "            X_train = train_images_26_w_dummy[i:i+batch]\n",
        "            Y_train = train_labels_26_w_dummy[i:i+batch]\n",
        "            #activating function for prediction\n",
        "            label = 1/( (1 + np.exp(-(X_train @ theta))))\n",
        "            m = len(label)\n",
        "            #calculate gradience\n",
        "            gradient = (X_train.T @ (label - Y_train ) / m)\n",
        "            #update theta\n",
        "            theta = theta - (alpha * gradient)\n",
        "            #calculte training liss\n",
        "            train_loss = np.mean(Y_train != (label >= 0.5))\n",
        "  #Convergence Condition\n",
        "  if(np.linalg.norm(gradient) < bgd_conv):\n",
        "    print(\" Gradient is below threshold value,breaking\",np.linalg.norm(gradient))\n",
        "    break\n",
        "  print(\"Epoch: \", epoch)\n",
        "  epoch = epoch +1\n",
        "\n",
        "#----------------- BGD on test data\n",
        "epoch = 0\n",
        "while(True):\n",
        "  #slice testing data in batches of 50\n",
        "  for i in range(0, rows, batch):\n",
        "            X_test = test_images_26_w_dummy[i:i+batch]\n",
        "            Y_test = test_labels_26[i:i+batch]\n",
        "            #activating function for prediction\n",
        "            label = 1/( (1 + np.exp(-(X_test @ theta))))\n",
        "            m = len(label)\n",
        "            #calculate gradience \n",
        "            gradient = (X_test.T @ (label - Y_test ) / m)\n",
        "            #update theta\n",
        "            theta = theta - (alpha * gradient)\n",
        "            #calculte training liss\n",
        "            test_loss = np.mean(Y_test != (label >= 0.5))\n",
        "\n",
        "    #Convergence Condition\n",
        "  if(np.linalg.norm(gradient) < bgd_conv):\n",
        "    print(\" Gradient is below threshold value,breaking\",np.linalg.norm(gradient))\n",
        "    break\n",
        "  print(\"Epoch: \", epoch)\n",
        "  epoch = epoch +1\n",
        "print(\"Training loss: \", train_loss)\n",
        "print(\"Testing loss: \", test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SinvIM8PCvQh",
        "outputId": "25dd00b8-c6e8-494f-8d7d-49528b195592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            " Gradient is below threshold value,breaking 4.4451078788515684e-11\n",
            "Epoch:  0\n",
            " Gradient is below threshold value,breaking 1.6707369830086834e-05\n",
            "Training loss:  0.0\n",
            "Testing loss:  0.0\n"
          ]
        }
      ],
      "source": [
        "# # Task 1.2) Stochastic Gradient Descent\n",
        "\n",
        "rows = train_images_26_w_dummy.shape[0]\n",
        "colms = train_images_26_w_dummy.shape[1]\n",
        "alpha = 0.4\n",
        "theta = np.zeros(colms)\n",
        "batch = 50\n",
        "bgd_conv= 0.001\n",
        "epoch = 0\n",
        "#-------------SGD on train data\n",
        "while(True):\n",
        "  print(\"Epoch: \", epoch)\n",
        "  #slice training data in batches of 1\n",
        "  for i in range(rows):\n",
        "            X_train = train_images_26_w_dummy[i:i+1]\n",
        "            Y_train = train_labels_26_w_dummy[i:i+1]\n",
        "            #activating function for prediction\n",
        "            label = 1/( (1 + np.exp(-(X_train @ theta))))\n",
        "            #calculate gradience \n",
        "            gradient = (X_train.T @ (label - Y_train ))\n",
        "            if(i%100 == 0):\n",
        "              if(np.linalg.norm(gradient) < bgd_conv):\n",
        "                break\n",
        "            #update theta\n",
        "            theta = theta - (alpha * gradient)\n",
        "            #calculte training liss\n",
        "            train_loss = np.mean(Y_train != (label >= 0.5))\n",
        "            \n",
        "    #Convergence Condition\n",
        "  if(np.linalg.norm(gradient) < bgd_conv):\n",
        "    print(\" Gradient is below threshold value,breaking\",np.linalg.norm(gradient))\n",
        "    break\n",
        "  epoch = epoch +1\n",
        "\n",
        "#----------------- SGD on test data\n",
        "epoch = 0\n",
        "while(True):\n",
        "  print(\"Epoch: \", epoch)\n",
        "  #slice testing data in batches of 50\n",
        "  for i in range(rows):\n",
        "            X_test = test_images_26_w_dummy[i:i+1]\n",
        "            Y_test = test_labels_26[i:i+1]\n",
        "            #activating function for prediction\n",
        "            label = 1/( (1 + np.exp(-(X_test @ theta))))\n",
        "            #calculate gradience \n",
        "            gradient = (X_test.T @ (label - Y_test ))\n",
        "            if(i%100 == 0):\n",
        "              if(np.linalg.norm(gradient) < bgd_conv):\n",
        "                break\n",
        "            #update theta\n",
        "            theta = theta - (alpha * gradient)\n",
        "            #calculte training liss\n",
        "            test_loss = np.mean(Y_test != (label >= 0.5))\n",
        "    #Convergence Condition\n",
        "  if(np.linalg.norm(gradient) < bgd_conv):\n",
        "    print(\" Gradient is below threshold value,breaking\",np.linalg.norm(gradient))\n",
        "    break\n",
        "\n",
        "  epoch = epoch +1\n",
        "print(\"Training loss: \", train_loss)\n",
        "print(\"Testing loss: \", test_loss)\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2RcQutJxO1E",
        "outputId": "a124a524-40e5-4539-8d50-fb0d9ae24df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            " Gradient is below threshold value,breaking 4.4451078788515684e-11\n",
            "Epoch:  0\n",
            " Gradient is below threshold value,breaking 3.6551559863564844e-11\n",
            "Training loss:  0.0\n",
            "Testing loss:  0.0\n"
          ]
        }
      ],
      "source": [
        "# Task 1.2.1 : Stochastic Gradient Descent\n",
        "rows = train_images_26_w_dummy.shape[0]\n",
        "colms = train_images_26_w_dummy.shape[1]\n",
        "alpha = 0.4\n",
        "bgd_conv = 0.00001\n",
        "epoch = 0\n",
        "theta = np.zeros(colms)\n",
        "#-------------SGD on train data\n",
        "while(True):\n",
        "  print(\"Epoch: \", epoch)\n",
        "  #slice training data in batches of 1\n",
        "  for i in range(rows):\n",
        "            X_train = train_images_26_w_dummy[i:i+1]\n",
        "            Y_train = train_labels_26_w_dummy[i:i+1]\n",
        "            #activating function for prediction\n",
        "            label = 1/( (1 + np.exp(-(X_train @ theta))))\n",
        "            #calculate gradience \n",
        "            gradient = (X_train.T @ (label - Y_train ))\n",
        "            if(i%100 == 0):\n",
        "              if(np.linalg.norm(gradient) < bgd_conv):\n",
        "                break\n",
        "            #update theta\n",
        "            theta = theta - (alpha * gradient)\n",
        "            #calculte training liss\n",
        "            train_loss = np.mean(Y_train != (label >= 0.5))\n",
        "            \n",
        "              \n",
        "  if(np.linalg.norm(gradient) < bgd_conv):\n",
        "    print(\" Gradient is below threshold value,breaking\",np.linalg.norm(gradient))\n",
        "    break\n",
        "  epoch = epoch +1\n",
        "\n",
        "#----------------- SGD on test data\n",
        "epoch = 0\n",
        "while(True):\n",
        "  print(\"Epoch: \", epoch)\n",
        "  #slice testing data in batches of 50\n",
        "  for i in range(rows):\n",
        "            X_test = test_images_26_w_dummy[i:i+1]\n",
        "            Y_test = test_labels_26[i:i+1]\n",
        "            #activating function for prediction\n",
        "            label = 1/( (1 + np.exp(-(X_test @ theta))))\n",
        "            #calculate gradience \n",
        "            gradient = (X_test.T @ (label - Y_test ))\n",
        "            if(i%100 == 0):\n",
        "              if(np.linalg.norm(gradient) < bgd_conv):\n",
        "                break\n",
        "            #update theta\n",
        "            theta = theta - (alpha * gradient)\n",
        "            #calculte training liss\n",
        "            test_loss = np.mean(Y_test != (label >= 0.5))\n",
        "\n",
        "  if(np.linalg.norm(gradient) < bgd_conv):\n",
        "    print(\" Gradient is below threshold value,breaking\",np.linalg.norm(gradient))\n",
        "    break\n",
        "\n",
        "  epoch = epoch +1\n",
        "print(\"Training loss: \", train_loss)\n",
        "print(\"Testing loss: \", test_loss)\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6GZpv07_Bq8M",
        "outputId": "4c9a55b0-9993-4e5a-a9d5-45532bf53c1d"
      },
      "outputs": [],
      "source": [
        "# Task 1.1.1 : Batch Gradient Descent\n",
        "rows = train_images_26_w_dummy.shape[0]\n",
        "colms = train_images_26_w_dummy.shape[1]\n",
        "alpha = 0.4\n",
        "bgd_conv = 0.00001\n",
        "epoch = 0\n",
        "theta = np.zeros(colms)\n",
        "batch = 50\n",
        "#-------------BGD on train data\n",
        "while(True):\n",
        "  #slice training data in batches of 50\n",
        "  for i in range(0, rows, batch):\n",
        "            X_train = train_images_26_w_dummy[i:i+batch]\n",
        "            Y_train = train_labels_26_w_dummy[i:i+batch]\n",
        "            #activating function for prediction\n",
        "            label = 1/( (1 + np.exp(-(X_train @ theta))))\n",
        "            m = len(label)\n",
        "            #calculate gradience \n",
        "            gradient = (X_train.T @ (label - Y_train ) / m)\n",
        "            #update theta\n",
        "            theta = theta - (alpha * gradient)\n",
        "            #calculte training liss\n",
        "            train_loss = np.mean(Y_train != (label >= 0.5))\n",
        "\n",
        "  if(np.linalg.norm(gradient) < bgd_conv):\n",
        "    print(\" Gradient is below threshold value,breaking\",np.linalg.norm(gradient))\n",
        "    break\n",
        "  print(\"Epoch: \", epoch, \"Gradient value: \", np.linalg.norm(gradient))\n",
        "  epoch = epoch +1\n",
        "\n",
        "##Results: Epoch:  12800 Train Loss:  0.0 Gradient value:  0.0002495867661348127 Does not converge\n",
        "#----------------- BGD on test data\n",
        "epoch = 0\n",
        "while(True):\n",
        "  #slice testing data in batches of 50\n",
        "  for i in range(0, rows, batch):\n",
        "            X_test = test_images_26_w_dummy[i:i+batch]\n",
        "            Y_test = test_labels_26[i:i+batch]\n",
        "            #activating function for prediction\n",
        "            label = 1/( (1 + np.exp(-(X_test @ theta))))\n",
        "            m = len(label)\n",
        "            #calculate gradience \n",
        "            gradient = (X_test.T @ (label - Y_test ) / m)\n",
        "            #update theta\n",
        "            theta = theta - (alpha * gradient)\n",
        "            #calculte training liss\n",
        "            test_loss = np.mean(Y_test != (label >= 0.5))\n",
        "\n",
        "\n",
        "  if(np.linalg.norm(gradient) < bgd_conv):\n",
        "    print(\" Gradient is below threshold value,breaking\",np.linalg.norm(gradient))\n",
        "    break\n",
        "  print(\"Epoch: \", epoch)\n",
        "  epoch = epoch +1\n",
        "  print(\"Epoch: \", epoch, \"Gradient value: \", np.linalg.norm(gradient))\n",
        "print(\"Training loss: \", train_loss)\n",
        "print(\"Testing loss: \", test_loss)\n",
        "\n",
        "###Result :Epoch:  13001 Gradient value:  0.0005241686073484014 the gradient value start saturating after epoch 130000. Does not converge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNwLXhb3kmfF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
